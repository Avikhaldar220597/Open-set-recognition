import sklearn.metrics
import numpy as np
import libmr

def fit_psi(X, y, tau, Cl):
    # obtain samples of class current
    l = np.argwhere(y == Cl).reshape(-1)
    X_l = X[l]

    # samples obtained from other classes
    m = np.argwhere(y != Cl).reshape(-1)
    X_m = X[m]

    # calculates the distance between pair of samples
    # current class, X_l, and samples of other classes, X_m
    D = sklearn.metrics.pairwise.pairwise_distances(X_l, X_m)

    psi = []

    # for each sample belonging to class Cl, esteem the
    # shape and scale parameters based on the half of the distance
    # of the tau samples not belonging to Cl
    for i in range(X_l.shape[0]):
        mr = libmr.MR()
        mr.fit_low(1/2 * D[i], tau)
        psi.append(mr)
    
    return np.array(psi)

def probability(psi, distance):
    return psi.w_score(distance)

def set_cover(X_l, psi, sigma):
    D = sklearn.metrics.pairwise.pairwise_distances(X_l) #distance matrix

    Nl = X_l.shape[0]
    U = range(Nl)
    S = np.zeros((Nl, Nl)) #return the array filled with zeroes with shape

    # runs through all the sample and finds if the probability
    # generated by the psi function between the distance of each pair 
    # of points is greater than or equal to sigma
    # for each point in the universe, a mapping is done with all other points
    for i in U:
        for j in U:
            if probability(psi[i], D[i, j]) >= sigma:
                S[i, j] = 1 #those points satisfying the condition becomes retained and its value becomes true i.e. 1
    
    C = []
    I = []

    # while the points represented by the values
    # donot cover the universal samples
    while (set(C) != set(U)):
        # manupulations necessary for the matrix
        # difference only contains 1 when there is a 
        # not yet covered and represented
        # for an extreme candidate value
        aux=np.zeros(Nl)
        aux[C]=1
        difference=S-np.matlib.repmat(aux,Nl,1)
        difference[I]=np.zeros((len(I),Nl))
        difference[:,I]=np.zeros((Nl,len(I)))
        difference=np.clip(difference,0,1)
        
        #obtain the extreme value having the largest quantity
        # of points not yet covered
        ind=np.argmax(np.sum(difference,axis=1),axis=0)

        # adds the new points covered
        C=np.append(C,np.asarray(np.where(S[ind])).reshape(-1))
        C=C.astype(int)
        C=np.unique(C)

        # adds the new extreme values to the set
        I.append(ind)
    
    return np.asarray(I).reshape(-1).tolist()

def test_EVM(EVs_psi, EVs_X, EVs_y, X_test, delta):
    # number of test and extreme vector samples
    M = X_test.shape[0]
    L = EVs_X.shape[0]

    # y_hat contains predictions that will be generated by the model; if
    # no extreme vector generate a probabitlity of belonging to the class
    # greater than or equal to delta, the sample shall be classified as 
    # to none of the classes seen during training, represented by -1
    y_hat = - np.ones(M)

    # distance matrix between extreme vectors and test samples
    D = sklearn.metrics.pairwise.pairwise_distances(EVs_X, X_test)

    # for each test sample
    for m in range(0, M):
        maior_probability = 0

        # ...though the extreme vectors, is it possible to
        # the sample to the distribution represented by each EV
        for l in range(0, L):
            prob = probability(EVs_psi[l], D[l, m])

            # if the inclusion probability is greater than delta or greater than
            # the highest probability obtained until then, stores the probabilty
            # and classifies the test sample with the same EV class
            if prob >= delta and prob > maior_probability:
                maior_probability = prob
                y_hat[m] = EVs_y[l]

    return y_hat

def train_EVM(X, y, tau, sigma):
    # set of existing classes
    C = np.unique(y)

    # lists with the information of extreme vectors of each class
    EVs_psi = []
    EVs_X = []
    EVs_y = []

    # for each class in the set
    for Cl in C:
        # trains the sample, fitting a vector of distribution functions
        # of probability for each class
        psi = fit_psi(X, y, tau, Cl)

        # obtain samples for current classes
        l = np.argwhere(y == Cl).reshape(-1)
        X_l = X[l]
        y_l = y[l]

        # eliminate the vectors of probability distribution function
        # unnecessary, leaving only the extreme vectors
        I = set_cover(X_l, psi, sigma)
        
        # adds the information from extreme vectors into lists
        if len(EVs_X) > 0:
            EVs_X = np.concatenate((EVs_X, X_l[I]), axis=0)
            EVs_y = np.concatenate((EVs_y, y_l[I]), axis=0)
            EVs_psi = np.concatenate((EVs_psi, psi[I]), axis=0)            
        else:
            EVs_X = X_l[I]
            EVs_y = y_l[I]            
            EVs_psi = psi[I]
    
    return (EVs_psi, np.asarray(EVs_X), np.asarray(EVs_y))